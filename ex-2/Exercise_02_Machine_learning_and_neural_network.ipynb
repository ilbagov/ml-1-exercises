{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning and neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please install the following packages in your virtaul environment by running the cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Image Classifier\n",
    "Images are represented as 3D arrays of numbers, withintegers between [0, 255]. E.g. 300 x 100 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, HTML, display\n",
    "\n",
    "Image(url= \"https://i.imgur.com/BAtqITz.jpg\", width=500) # from Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred():\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. As we present (an inexhaustive) list of challenges below, keep in mind the raw representation of images as a 3-D array of brightness values:\n",
    "\n",
    "* **Viewpoint variation**. A single instance of an object can be oriented in many ways with respect to the camera.\n",
    "Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).\n",
    "* **Deformation.** Many objects of interest are not rigid bodies and can be deformed in extreme ways.\n",
    "* **Occlusion.** The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.\n",
    "* **Illumination conditions.** The effects of illumination are drastic on the pixel level.\n",
    "* **Background clutter.** The objects of interest may blend into their environment, making them hard to identify.\n",
    "* **Intra-class variation.** The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://i.imgur.com/j5jPEv0.jpg\", width=800) # from Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is the use of data driving methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Tensorflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://i.imgur.com/4nk5b4c.jpg\", width=700) # from Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is the flow of tensors in a computational graph.\n",
    "* Library for defining computation graphs\n",
    "* Calculating gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "TensorFlow does have its own data structure for the purpose of performance and ease of use. Tensor is the data structure used in Tensorflow. You can think of a TensorFlow tensor as an n-dimensional array or list.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Tensors have a Shape thatâ€™s described with a vectorn $[ 10000, 256, 256, 3 ]$\n",
    "* 10000 Images\n",
    "* Each Image has 256 Rows\n",
    "* Each Row has 256 Pixels\n",
    "* Each Pixel has 3 channels (RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computaional Graph\n",
    "\n",
    "The biggest idea about Tensorflow is that all the numerical computations are expressed as a computational graph. In other words, the backbone of any Tensorflow program is a Graph. Anything that happens in your model is represented by the computational graph.  \n",
    "\n",
    "Computational graphs are an abstract way of describing computations as directed graph: \n",
    "* The edges correspond to multidimensional arrays (Tensors). \n",
    "* The nodes create or manipulate these Tensors according to specific rules (Operations Ops)\n",
    "  * Operations on tensors (like math operations) \n",
    "  * Generating tensors (like variables and constants). \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://i.imgur.com/2Ys4yTu.jpg\", width=650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this model we will use `Keras`. The `Sequential` in `Keras` model is a linear stack of layers.\n",
    "\n",
    "You can create a Sequential model by passing a list of layer instances to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn import datasets\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras models\n",
    "The model object is how you tell Keras where the model starts and stops: where data comes in and where predictions come out. Build the tf.keras.Sequential model by stacking layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://i.imgur.com/GtL0Ehr.jpg\", width=850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 Architecture\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(6, (5, 5), activation='relu', input_shape=(32, 32, 1), name= \"Convolution_1\"),\n",
    "    layers.MaxPooling2D((2, 2), name= \"Subsampling_1\"),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu', name= \"Convolution_2\"),\n",
    "    layers.MaxPooling2D((2, 2) ,name= \"Subsampling_2\"),\n",
    "    layers.Flatten(name= \"FullyConnection_1\"),\n",
    "    layers.Dense(84, activation='relu', name= \"FullyConnection_2\"),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='LeNet-5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the model\n",
    "\n",
    "The summary will tell you the names of the layers, as well as how many units they have and how many parameters are in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a model\n",
    "\n",
    "The plot will show how the layers connect to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import plot_model\n",
    "tf.keras.utils.plot_model(model, to_file ='model.png')\n",
    "from matplotlib import pyplot as plt\n",
    "img = plt.imread('model.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression:\n",
    "### Providing the Data\n",
    "\n",
    "Next up we'll feed in some data. The relationship between $x$ and $y$ is that\n",
    "$$y = x^3 - 4x^2 - 2^x + 2 + Noise $$\n",
    "\n",
    "A python library called 'Numpy' provides lots of array type data structures that are a standard way of doing it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 10, 0.16)\n",
    "y = 0.6*x**3 - 5.2*x**2 - 3*x + 2\n",
    "y_noise = y + np.random.normal(0, 1.8, size=(len(x),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x, y_noise, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y_noise, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"The number of training data is: \", x_train.shape[0])\n",
    "print(\"The number of validation data is: \", x_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Training Data\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At the Beginning: the Neuron\n",
    "Making the comparison to the original, biological neurons, we explained in the lecture how scientists developed a simple model to simulate their behavior$^1$.\n",
    "\n",
    "The mathematical equation for a neuron\n",
    "$$ y = f(wx) $$\n",
    "\n",
    "where:\n",
    "* $x$ : input\n",
    "* $f$ : activation function\n",
    "* $w$ : wieght\n",
    "* $y$ : output\n",
    "\n",
    "Assuming that there is no activation function, the neuron can approximate a linear function.\n",
    "$$ y = wx $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://i.imgur.com/KKYGVtc.jpg\", width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [layers.Dense(input_shape=[1], units= 1,activation=None,use_bias=False)],\n",
    "    name = 'linear') \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the computer is trying to 'learn' that, it makes a guess,maybe y=10x. The **LOSS function** measures the guessed answers against the known correct answers and measures how well or how badly it did.\n",
    "\n",
    "It then uses the **OPTIMIZER function** to make another guess. Based on how the loss function went, it will try to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[keras.metrics.MSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training the neural network, where it 'learns' the relationship between the $x$ and $y$ is in the `model.fit` call. This is where it will go through the loop making a guess, measuring how good or bad it is, using the opimizer to make another guess etc. It will do it for the number of epochs you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=500, batch_size=47, validation_data=(x_val, y_val))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we can reduce the loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of bias:\n",
    "We will create a neural network. It has also 1 layer, and that layer has 1 neuron and bias. The model that can be represented by this network is: \n",
    "$$ y = wx + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://i.imgur.com/lPDTbj0.jpg\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(input_shape=[1], units= 1,use_bias=True)])\n",
    "                          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MSE\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "metrics=[keras.metrics.MSE]\n",
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=500, batch_size=47, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of initializers\n",
    "\n",
    "Initializations define the way to set the initial random weights of Keras layers.\n",
    "\n",
    "The keyword arguments used for passing initializers to layers will depend on the layer. Usually it is simply `kernel_initializer` and `bias_initializer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(input_shape=[1], units= 1,use_bias=True,\n",
    "                                      kernel_initializer = keras.initializers.Constant(value=-20. ),\n",
    "                                      bias_initializer = keras.initializers.Constant(value= -14.),\n",
    "                                      )])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MSE\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "metrics=[keras.metrics.MSE]\n",
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=500, batch_size=47, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Try to initialize the model with the weight and bias, which can make the loss as low as possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of other loss function\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "It is perhaps the most simple and common metric for regression evaluation. It is defined by the equation:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N} \\sum^{N}_{i=1}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "In MAE the error is calculated as an average of absolute differences between the target values and the predictions. The MAE is a linear score which means that all the individual differences are weighted equally in the average. Mathematically, it is calculated using this formula:\n",
    "$$\\text{MAE} = \\frac{1}{N} \\sum^{N}_{i=1}|y_i - \\hat{y}_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(input_shape=[1], units= 1,activation=None,use_bias=True)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MAE\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "metrics=[keras.metrics.MAE]\n",
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=500, batch_size=47, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: MAE or MSE?**\n",
    "* Do you have outliers in the data?\n",
    "    * Use MAE\n",
    "\n",
    "* Are you sure they are outliers?\n",
    "    * Use MAE\n",
    "\n",
    "* or they are just unexpected values we should still care about?\n",
    "    * Use MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of multi-layers network\n",
    "Perhaps the use of multi-layers network can help us to fit the function better. The model that can be represented by this network is: \n",
    "$$y = w_3(w_2(w_1x + b_1)+ b_2)+ b_3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://i.imgur.com/q4ozgnk.jpg\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(input_shape=[1],units= 1,use_bias=True),\n",
    "                         layers.Dense(units= 1,use_bias=True),\n",
    "                         layers.Dense(units= 1,use_bias=True)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MSE\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "metrics=[keras.metrics.MSE]\n",
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=500, batch_size=47, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: We used multi-layer networks, but the model is still linear, why is that? Can we say that this network is a deep network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of activation function:\n",
    "However, simply outputting a weighted sum of the inputs limits the tasks that can be performed by the neural network. Therefore, a better processing of the data would be to map the weighted sum to a nonlinear space.\n",
    "\n",
    "ReLU: An activation function that allows a model to solve nonlinear problems. The model that can be represented by this network is: \n",
    "$$y = \\max(w_1x + b_1,0) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(input_shape=[1], units= 1,activation='relu',use_bias=True)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MSE\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "metrics=[keras.metrics.MSE]\n",
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=500, batch_size=47, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Why is the result worse than the linear model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal approximation theroy: \n",
    "The universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of $R^n$, under mild assumptions on the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://i.imgur.com/BLmMNo4.jpg\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(input_shape=[1], units= 12,use_bias=True,activation='relu'),\n",
    "                         layers.Dense(units= 1,use_bias=False)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MSE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "metrics=[keras.metrics.MSE]\n",
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=1500, batch_size=47, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(input_shape=[1], units= 20,use_bias=True,activation='relu'),\n",
    "                         layers.Dense(units= 10,use_bias=True,activation='relu'),\n",
    "                         layers.Dense(units= 10,use_bias=True,activation='relu'),\n",
    "                         layers.Dense(units= 1,use_bias=True)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MAE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "metrics=[keras.metrics.MAE]\n",
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=2200, batch_size=47, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([-1.5,-1,-0.5,-6.6,7.5])\n",
    "ys = xs**3 - 4*xs**2 - 2*xs + 2\n",
    "ys_noise = ys + np.random.normal(0, 1.5, size=(len(xs),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (10,6))\n",
    "plt.scatter(xs, ys_noise, label = \"Ground truth + noise\", color = 'green')\n",
    "plt.scatter(xs,model.predict(xs), label = \"Prediction\", color = 'blue')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (10,6))\n",
    "plt.scatter([4],model.predict([4]), label = \"Test data\", color = 'green')\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x, y_noise, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement of generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "L2 regularization is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective\n",
    "\n",
    "http://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(input_shape=[1], units= 20,use_bias=True,activation='relu',\n",
    "                                      kernel_regularizer=keras.regularizers.l2(0.25)),\n",
    "                         layers.Dense(units= 10,use_bias=True,activation='relu',\n",
    "                                     kernel_regularizer=keras.regularizers.l2(0.25)),\n",
    "                         layers.Dense(units= 10,use_bias=True,activation='relu',\n",
    "                                     kernel_regularizer=keras.regularizers.l2(0.3)),\n",
    "                         layers.Dense(units= 1,use_bias=True)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MAE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "metrics=[keras.metrics.MAE]\n",
    "model.compile(loss=loss, # Loss function to minimize \n",
    "              optimizer=optimizer,# Optimizer\n",
    "              merics = metrics) # List of metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=2200, batch_size=47, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(x,y, label = \"Ground truth\", color = 'black')\n",
    "plt.scatter(x_train, y_train, label = \"Ground truth + noise\", color = 'black')\n",
    "plt.scatter(x_val, y_val, label = \"Test Data\", color = 'green')\n",
    "plt.plot(x, model.predict(x), color = 'red', ls = '--', lw = 3, \n",
    "         label = 'Fitted function')\n",
    "plt.legend(fontsize = 16, loc = 'upper left')\n",
    "plt.xlabel ('x', fontsize = 16)\n",
    "plt.ylabel ('y', fontsize = 16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "fig = plt.figure (figsize = (12,6))\n",
    "plt.plot(history.history['loss'], label = \"train_loss\", color = 'black')\n",
    "plt.legend(fontsize = 16, loc = 'upper right')\n",
    "#print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(url='./image67.gif',width=500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py36_tf2",
   "language": "python",
   "name": "venv_py36_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
